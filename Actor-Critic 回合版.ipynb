{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "#GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04174217, -0.00824811, -0.02843118, -0.04435518])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1').unwrapped\n",
    "env.action_space.n\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4827  0.5173\n",
       "[torch.cuda.FloatTensor of size 1x2 (GPU 0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ini_net(md):\n",
    "    for m in md.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_normal(m.weight.data)\n",
    "            torch.nn.init.normal(m.bias.data)\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(4,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,2),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return x\n",
    "    \n",
    "actor_net = ActorNet().cuda()\n",
    "actor_net.apply(ini_net)\n",
    "testx = Variable(torch.randn(1,4)).cuda()\n",
    "actor_net(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.9699\n",
       "[torch.cuda.FloatTensor of size 1x1 (GPU 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CriticNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(4,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return x\n",
    "       \n",
    "critic_net = CriticNet().cuda()\n",
    "critic_net.apply(ini_net)\n",
    "testx = Variable(torch.randn(1,4)).cuda()\n",
    "critic_net(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, Variable containing:\n",
       " -0.5598\n",
       " [torch.cuda.FloatTensor of size 1 (GPU 0)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_action(state): \n",
    "    prob = actor_net(state)\n",
    "    m = Categorical(prob)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    action = action.data.cpu().numpy()[0]\n",
    "    return action, log_prob\n",
    "\n",
    "testx = Variable(torch.randn(1,4)).cuda()\n",
    "select_action(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self):\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.clips = []\n",
    "        self.R = 0\n",
    "    \n",
    "    def save_log_probs(self, log_prob): #存储一个回合的log(prob)\n",
    "        self.log_probs.append(log_prob)\n",
    "    \n",
    "    def save_rewards(self, r): #存储一个回合的reward\n",
    "        self.rewards.append(r)\n",
    "    \n",
    "    #def save_action(self, a): #存储一个回合的reward\n",
    "    #    self.actions.append(a)\n",
    "        \n",
    "    def save_clip(self,s): #存储一个s1和s2\n",
    "        self.clips.append(s)\n",
    "        \n",
    "    def _reward(self): #重定义reward\n",
    "        n = len(self.rewards)\n",
    "        ds = np.zeros(n)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, n)):\n",
    "            running_add = running_add * 0.99 + self.rewards[t]\n",
    "            ds[t] = running_add\n",
    "        return FloatTensor(ds)\n",
    "    \n",
    "    def critic_loss(self):\n",
    "        vt = self._reward()\n",
    "        As = []\n",
    "        closs = 0\n",
    "        for t in range(len(vt)):\n",
    "            #a = self.actions[t]\n",
    "            s = self.clips[t]\n",
    "            v = vt[t] #实际的奖励v\n",
    "            Qs = critic_net(s) #估计的奖励\n",
    "            A = v - Qs #实际的奖励-估计的奖励\n",
    "            As.append(A.detach())\n",
    "            closs += A.norm()     \n",
    "        return As, closs\n",
    "       \n",
    "    def optimize(self):\n",
    "        n = len(self.rewards)\n",
    "        aloss = 0\n",
    "        As,closs = self.critic_loss()\n",
    "        #print(As)\n",
    "        for prob,A in zip(self.log_probs, As):\n",
    "            #print('a:{}\\n prob:{}'.format(r,prob.data))\n",
    "            aloss += - prob * A #当前状态的loss函数\n",
    "            #print(\"loss\",loss)\n",
    "        aloss /= n\n",
    "        closs /= n\n",
    "        return aloss, closs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "num_episodes = 20000\n",
    "optimizer1 = optim.Adam(critic_net.parameters(),lr=1e-4)\n",
    "optimizer2 = optim.Adam(actor_net.parameters(),lr=1e-5)\n",
    "\n",
    "def get_state():\n",
    "    state = FloatTensor(env.state).cuda().view(1,-1)\n",
    "    state = Variable(state)\n",
    "    return state\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 初始化环境\n",
    "    env.reset()\n",
    "    s = get_state()\n",
    "    step = 0\n",
    "    ept = Episode()\n",
    "    while True: #无限循环\n",
    "        #env.render()\n",
    "        \n",
    "        a, log_prob = select_action(s) #根据状态选择一个动作，得到a和log_prob\n",
    "        _, r, done, _ = env.step(a) #计算该动作的下一个状态，奖励，done   \n",
    "        \n",
    "        if done: r -= 20\n",
    "        \n",
    "        ept.save_log_probs(log_prob)\n",
    "        ept.save_rewards(r)\n",
    "        ept.save_clip(s)\n",
    "        #ept.save_action(a)\n",
    "        \n",
    "        s = get_state()\n",
    "        step += 1\n",
    "        if done:\n",
    "            writer.add_scalar('step', step, i_episode)\n",
    "            #print(i_episode, step)\n",
    "            break\n",
    "    #回合结束后开始优化\n",
    "    aloss, closs = ept.optimize()\n",
    "    optimizer1.zero_grad\n",
    "    optimizer2.zero_grad\n",
    "    aloss.backward()\n",
    "    closs.backward()\n",
    "    optimizer2.step()\n",
    "    optimizer1.step()\n",
    "    \n",
    "    writer.add_scalar('aloss', aloss.data.cpu().numpy(), i_episode)\n",
    "    writer.add_scalar('closs', closs.data.cpu().numpy(), i_episode)\n",
    "    #print(\"loss:\",loss.data.cpu().numpy()[0])\n",
    "\n",
    "print('Complete')\n",
    "writer.close()\n",
    "#env.render(close=True)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if done: r-=20 很重要   \n",
    "\n",
    "不是特别好收敛，主要是closs还没有达到一个较好的状态，甚至是一个错误的值状态，aloss就收敛了。所以尽量要closs先训练一定程度后，aloss再跟上去\n",
    "\n",
    "学习率太高可能无法收敛   \n",
    "\n",
    "critic网络只输出一个状态，为什么不是每个动作都输出一个状态。\n",
    "因为c网络是判断某个状态下，期望得多少分数，而不是某个动作得多少分，以此来评价实际动作的效果；\n",
    "\n",
    "dropout可以用\n",
    "\n",
    "loss加均值？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
