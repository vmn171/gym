{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "#GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02722231, -0.02951582,  0.04844879, -0.00138314])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1').unwrapped\n",
    "env.action_space.n\n",
    "env.reset()\n",
    "env.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.6463  0.3537\n",
       "[torch.cuda.FloatTensor of size 1x2 (GPU 0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ini_net(md):\n",
    "    for m in md.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_normal(m.weight.data)\n",
    "            torch.nn.init.normal(m.bias.data)\n",
    "\n",
    "class PN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PN, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(4,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,2),\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return x\n",
    "    \n",
    "pnet = PN().cuda()\n",
    "pnet.apply(ini_net)\n",
    "testx = Variable(torch.randn(1,4)).cuda()\n",
    "pnet(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, Variable containing:\n",
      "-0.9122\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def select_action(state): \n",
    "    prob = pnet(state)\n",
    "    m = Categorical(prob)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    action = action.data.cpu().numpy()[0]\n",
    "    return action, log_prob\n",
    "\n",
    "testx = Variable(torch.randn(1,4)).cuda()\n",
    "print(select_action(testx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode 回合片段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self):\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.R = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def save_log_probs(self, log_prob): #存储一个回合的log(prob)\n",
    "        self.log_probs.append(log_prob)\n",
    "    \n",
    "    def save_rewards(self, r): #存储一个回合的reward\n",
    "        self.rewards.append(r)\n",
    "        \n",
    "    def _reward(self): #重定义reward\n",
    "        n = len(self.rewards)\n",
    "        ds = np.zeros(n)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, n)):\n",
    "            running_add = running_add * 0.99 + self.rewards[t]\n",
    "            ds[t] = running_add\n",
    "        ds = (ds - np.mean(ds))/np.std(ds)\n",
    "        return FloatTensor(ds)\n",
    "        \n",
    "    \n",
    "    def optimize(self):\n",
    "        vt = self._reward()\n",
    "        #print(vt)\n",
    "        loss = 0\n",
    "        t = 0\n",
    "        for prob,r in zip(self.log_probs, vt):\n",
    "            #print('a:{}\\n prob:{}'.format(r,prob.data))\n",
    "            loss += - prob * r #当前状态的loss函数\n",
    "            #print(\"loss\",loss)\n",
    "        return loss\n",
    "\n",
    "#ept = Episode(net, optimizer, gamma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "GAMMA = 1\n",
    "num_episodes = 5000\n",
    "optimizer = optim.Adam(pnet.parameters(),lr=1e-4)\n",
    "\n",
    "def get_state():\n",
    "    state = FloatTensor(env.state).cuda().view(1,-1)\n",
    "    state = Variable(state)\n",
    "    return state\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 初始化环境\n",
    "    env.reset()\n",
    "    state = get_state()\n",
    "    episode = Episode()\n",
    "    for t in count(): #无限循环\n",
    "        #if i_episode>3500: \n",
    "        env.render()\n",
    "        \n",
    "        action, log_prob = select_action(state) #选择一个动作\n",
    "        _, reward, done, _ = env.step(action) #计算该动作的奖励，done                \n",
    "        \n",
    "        episode.save_log_probs(log_prob)\n",
    "        episode.save_rewards(reward)\n",
    "   \n",
    "        state = get_state() #当前屏幕状态重新获取\n",
    "\n",
    "        if done:\n",
    "            writer.add_scalar('t', t, i_episode)\n",
    "            #print(i_episode, t)\n",
    "            break\n",
    "    #回合结束后开始优化\n",
    "    optimizer.zero_grad()\n",
    "    loss = episode.optimize()\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    writer.add_scalar('loss', loss, i_episode)\n",
    "    #print(\"loss:\",loss.data.cpu().numpy()[0])\n",
    "print('Complete')\n",
    "writer.close()\n",
    "#env.render(close=True)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 主要错误还是在loss函数的计算和r的引导上\n",
    "### 增加网络的泛化能力"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
