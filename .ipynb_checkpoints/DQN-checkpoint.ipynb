{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "#GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重放记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward')) #命名元组\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity): #初始化\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args): #添加transition\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity # %取余数，可以循环\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境和获得裁剪后的帧图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADKCAYAAACrHYtRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrc2jGcwmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183f98358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "env.reset()\n",
    "\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "screen_width = 600 #设置屏幕宽度，由每个env的render决定\n",
    "\n",
    "def get_cart_location():#获得小车的中间位置在屏幕坐标上位置\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0) \n",
    "\n",
    "def get_screen(): #获得修改后的屏幕图像\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))  # 转成Tensro格式 (CHW)\n",
    "    # 裁剪图片的上下\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # 进行左右切片\n",
    "    screen = screen[:, :, slice_range]\n",
    "    plt.imshow(screen.transpose((1,2,0)))\n",
    "    \n",
    "    #screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 #返回一个内存中的连续数组\n",
    "    screen = np.float32(screen) / 255 #好像也不用\n",
    "    screen = torch.from_numpy(screen) #32位的才能转换\n",
    "    # 调整大小(BCHW)\n",
    "    return resize(screen).unsqueeze(0).type(Tensor)\n",
    "\n",
    "get_screen()\n",
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state): #策略：e贪婪法选择动作的策略\n",
    "    global steps_done\n",
    "    sample = random.random() #产生一个随机数\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY) #一个衰减的eps\n",
    "    eps_threshold = EPS_END\n",
    "    steps_done += 1 \n",
    "    if sample > eps_threshold: #如果随机数大于eps\n",
    "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1) #Q网络输出的0和1\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]]) #否则随机选择0和1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 绘制片段持续时间\n",
    "横坐标是片段   \n",
    "纵坐标是持续时间   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.FloatTensor(episode_durations)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 采样100片段的平均值然后绘制\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "        print(means)\n",
    "\n",
    "    plt.pause(0.001)  # 每一帧暂停一会\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sync = 0\n",
    "\n",
    "def optimize_model():\n",
    "    global last_sync\n",
    "    if len(memory) < BATCH_SIZE: #如果记忆长度小于batch_size就返回\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE) #取一个batch_size长度的transitions\n",
    "    batch = Transition(*zip(*transitions)) #打一下包\n",
    "\n",
    "    # 非最终状态的掩码,也就是没结束的s-a-s，打上True标记\n",
    "    non_final_mask = tuple(map(lambda s: s is not None, batch.next_state))\n",
    "    non_final_mask = ByteTensor(non_final_mask)\n",
    "\n",
    "    # 我们不希望反向传播 expected action values\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]),\n",
    "                                     volatile=True)\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "\n",
    "\n",
    "    state_action_values = model(state_batch).gather(1, action_batch) #计算Q(s,a)\n",
    "\n",
    "    # 对所有nest_stats计算 V(s_{t+1}) \n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor)) #全0\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0] #取最大的一个，动作\n",
    "    # Now, we don't want to mess up the loss with a volatile flag, so let's\n",
    "    # clear it. After this, we'll just end up with a Variable that has\n",
    "    # requires_grad=False\n",
    "    next_state_values.volatile = False\n",
    "    # expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # loss\n",
    "    #loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    loss = torch.nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #for param in model.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 200\n",
    "num_episodes = 500\n",
    "\n",
    "model = DQN().cuda()\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters())\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "memory = ReplayMemory(100000) #10000个重放记忆\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-9b71f40c1e3d>\", line 12, in <module>\n",
      "    _, reward, done, _ = env.step(action[0, 0]) #计算该动作的奖励，done\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/inspect.py\", line 736, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/inspect.py\", line 706, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/posixpath.py\", line 376, in abspath\n",
      "    return normpath(path)\n",
      "  File \"/Users/lhan/anaconda3/lib/python3.6/posixpath.py\", line 357, in normpath\n",
      "    new_comps.append(comp)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 初始化环境\n",
    "    env.reset()\n",
    "    last_screen = get_screen() #获得环境的图像\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count(): #无限循环\n",
    "        action = select_action(state) #选择一个动作\n",
    "        _, reward, done, _ = env.step(action[0, 0]) #计算该动作的奖励，done\n",
    "        reward = Tensor([reward]) #存储奖励\n",
    "\n",
    "        # 观察状态\n",
    "        last_screen = current_screen #上一个屏幕状态等于当前屏幕状态\n",
    "        current_screen = get_screen() #当前屏幕状态重新获取\n",
    "        if not done: #如果没有结束\n",
    "            next_state = current_screen - last_screen #下一个状态位当前屏幕状态-上一个屏幕状态\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # 把transtion存入重放记忆\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 变成下一个状态\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        for i in range(1):\n",
    "            loss = optimize_model()\n",
    "        if done:\n",
    "            #episode_durations.append(t + 1)\n",
    "            #plot_durations()\n",
    "            writer.add_scalar('t', t, i_episode)\n",
    "            #print(i_episode, t)\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "writer.close()\n",
    "#env.render(close=True)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
