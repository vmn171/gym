{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "#GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重放记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward')) #命名元组\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity): #初始化\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args): #添加transition\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity # %取余数，可以循环\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1043.0883   138.2366\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ini_net(md):\n",
    "    for m in md.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal(m.weight.data)\n",
    "            #torch.nn.init.xavier_normal(m.bias.data)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(4,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,2),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.MLP(x)\n",
    "        return x\n",
    "    \n",
    "net = DQN()\n",
    "net.apply(ini_net)\n",
    "testx = Variable(torch.randn(1,4))\n",
    "net(testx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动作策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(state): #策略：e贪婪法选择动作的策略\n",
    "    global steps_done\n",
    "    sample = random.random() #产生一个随机数\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY) #一个衰减的eps\n",
    "    eps_threshold = EPS_END\n",
    "    steps_done += 1 \n",
    "    if sample > eps_threshold: #如果随机数大于eps\n",
    "        return model(Variable(state, volatile=True).type(FloatTensor)).data.max(1)[1].view(1, 1) #Q网络输出的0和1\n",
    "    else:\n",
    "        return LongTensor([[random.randrange(2)]]) #否则随机选择0和1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 值迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_sync = 0\n",
    "\n",
    "def optimize_model():\n",
    "    global last_sync\n",
    "    if len(memory) < BATCH_SIZE: #如果记忆长度小于batch_size就返回\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE) #取一个batch_size长度的transitions\n",
    "    batch = Transition(*zip(*transitions)) #打一下包\n",
    "\n",
    "    # 非最终状态的掩码,也就是没结束的s-a-s，打上True标记\n",
    "    non_final_mask = tuple(map(lambda s: s is not None, batch.next_state))\n",
    "    non_final_mask = ByteTensor(non_final_mask)\n",
    "\n",
    "    # 我们不希望反向传播 expected action values\n",
    "    non_final_next_states = Variable(torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None]),\n",
    "                                     volatile=True)\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "\n",
    "\n",
    "    state_action_values = model(state_batch).gather(1, action_batch) #计算Q(s,a)\n",
    "\n",
    "    # 对所有nest_stats计算 V(s_{t+1}) \n",
    "    next_state_values = Variable(torch.zeros(BATCH_SIZE).type(Tensor)) #全0\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0] #取最大的一个，动作\n",
    "    next_state_values.volatile = False\n",
    "    # expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber loss\n",
    "    #loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    loss = torch.nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #for param in model.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 640\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 200 #越大衰减越慢\n",
    "num_episodes = 5000\n",
    "\n",
    "model = DQN().cuda()\n",
    "model.apply(ini_net)\n",
    "#optimizer = optim.RMSprop(model.parameters())\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-2)\n",
    "memory = ReplayMemory(100000) #10000个重放记忆\n",
    "pool = ReplayMemory(640) #重放记忆缓冲池\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "env.reset()\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def get_screen(): #获得修改后的屏幕图像\n",
    "    state = FloatTensor(env.state).cuda().view(1,-1)\n",
    "    return state\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # 初始化环境\n",
    "    env.reset()\n",
    "    last_screen = get_screen() #获得环境的图像\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count(): #无限循环\n",
    "        env.render()\n",
    "        action = select_action(state) #选择一个动作\n",
    "        _, reward, done, _ = env.step(action[0, 0]) #计算该动作的奖励，done\n",
    "        \n",
    "        #reward += 0.5 * np.abs(1-env.state[0])\n",
    "        if action == 1 and env.state[0]>0:\n",
    "            reward -= 1\n",
    "        if action == -1 and env.state[0]<0:\n",
    "            reward -= 1\n",
    "        \n",
    "        reward = Tensor([reward]) #存储奖励\n",
    "\n",
    "        # 观察状态\n",
    "        last_screen = current_screen #上一个屏幕状态等于当前屏幕状态\n",
    "        current_screen = get_screen() #当前屏幕状态重新获取\n",
    "        if not done: #如果没有结束\n",
    "            next_state = current_screen - last_screen #下一个状态位当前屏幕状态-上一个屏幕状态\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # 把transtion存入重放记忆\n",
    "        pool.push(state, action, next_state, reward)\n",
    "        # 变成下一个状态\n",
    "        state = next_state\n",
    "\n",
    "        for i in range(1):\n",
    "            loss = optimize_model()\n",
    "        if done:\n",
    "            writer.add_scalar('t', t, i_episode)\n",
    "            #print(i_episode, t)\n",
    "            break\n",
    "    for t in pool.memory:#把缓冲池的内容放进记忆库\n",
    "        memory.push(*t)\n",
    "\n",
    "print('Complete')\n",
    "writer.close()\n",
    "#env.render(close=True)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个问题，如何防止溜边，二如何保证战果\n",
    "\n",
    "增大批次，增大记忆库，简单粗暴提神\n",
    "溜边和直立存着矛盾约束，要减少溜边可能需要多步骤策略\n",
    "\n",
    "现在判定改变reward有无效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
